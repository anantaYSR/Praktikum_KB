{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Liblary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi Sederhana pada DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6], dtype=float)\n",
    "y = np.array([2,4,6,8,10,12], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menggunakan parameter jika nilai yang di dapat sdh di tentukan maka epoch berhenti \n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('loss') < 1e-4):\n",
    "            self.model.stop_training = True\n",
    "cb = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## latih Model nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 2.4666\n",
      "Epoch 2/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1424\n",
      "Epoch 3/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5296\n",
      "Epoch 4/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2459\n",
      "Epoch 5/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1146\n",
      "Epoch 6/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0538\n",
      "Epoch 7/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0257\n",
      "Epoch 8/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0127\n",
      "Epoch 9/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 10/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 11/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 12/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 13/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 14/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 15/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 16/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 17/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 18/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 19/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 20/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 21/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 22/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 23/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 24/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 25/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 26/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 27/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 28/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 29/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 30/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 31/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 32/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 33/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 34/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 35/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 36/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 37/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 38/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 39/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 40/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 41/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 42/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 43/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 44/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 45/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 46/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 47/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 48/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 49/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 50/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 51/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 52/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 53/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 54/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 55/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0010\n",
      "Epoch 56/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 57/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 58/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 59/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 60/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9733e-04\n",
      "Epoch 61/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.9005e-04\n",
      "Epoch 62/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8285e-04\n",
      "Epoch 63/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7568e-04\n",
      "Epoch 64/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6858e-04\n",
      "Epoch 65/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6151e-04\n",
      "Epoch 66/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.5452e-04\n",
      "Epoch 67/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4756e-04\n",
      "Epoch 68/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4066e-04\n",
      "Epoch 69/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3380e-04\n",
      "Epoch 70/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2699e-04\n",
      "Epoch 71/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2025e-04\n",
      "Epoch 72/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1353e-04\n",
      "Epoch 73/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0688e-04\n",
      "Epoch 74/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0028e-04\n",
      "Epoch 75/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9371e-04\n",
      "Epoch 76/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8720e-04\n",
      "Epoch 77/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8074e-04\n",
      "Epoch 78/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7433e-04\n",
      "Epoch 79/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6796e-04\n",
      "Epoch 80/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6163e-04\n",
      "Epoch 81/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5536e-04\n",
      "Epoch 82/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.4913e-04\n",
      "Epoch 83/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4293e-04\n",
      "Epoch 84/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.3680e-04\n",
      "Epoch 85/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3070e-04\n",
      "Epoch 86/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2464e-04\n",
      "Epoch 87/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1864e-04\n",
      "Epoch 88/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1267e-04\n",
      "Epoch 89/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0675e-04\n",
      "Epoch 90/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0087e-04\n",
      "Epoch 91/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.9504e-04\n",
      "Epoch 92/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8925e-04\n",
      "Epoch 93/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8350e-04\n",
      "Epoch 94/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7780e-04\n",
      "Epoch 95/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7212e-04\n",
      "Epoch 96/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6651e-04\n",
      "Epoch 97/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6091e-04\n",
      "Epoch 98/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5537e-04\n",
      "Epoch 99/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4988e-04\n",
      "Epoch 100/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4440e-04\n",
      "Epoch 101/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3898e-04\n",
      "Epoch 102/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3360e-04\n",
      "Epoch 103/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2825e-04\n",
      "Epoch 104/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2294e-04\n",
      "Epoch 105/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1768e-04\n",
      "Epoch 106/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1246e-04\n",
      "Epoch 107/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0725e-04\n",
      "Epoch 108/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0211e-04\n",
      "Epoch 109/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9700e-04\n",
      "Epoch 110/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9191e-04\n",
      "Epoch 111/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8687e-04\n",
      "Epoch 112/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8187e-04\n",
      "Epoch 113/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7690e-04\n",
      "Epoch 114/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7198e-04\n",
      "Epoch 115/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6707e-04\n",
      "Epoch 116/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6222e-04\n",
      "Epoch 117/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5739e-04\n",
      "Epoch 118/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5259e-04\n",
      "Epoch 119/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4784e-04\n",
      "Epoch 120/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4313e-04\n",
      "Epoch 121/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3844e-04\n",
      "Epoch 122/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3379e-04\n",
      "Epoch 123/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2917e-04\n",
      "Epoch 124/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2458e-04\n",
      "Epoch 125/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2004e-04\n",
      "Epoch 126/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1552e-04\n",
      "Epoch 127/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1103e-04\n",
      "Epoch 128/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0659e-04\n",
      "Epoch 129/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0216e-04\n",
      "Epoch 130/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9778e-04\n",
      "Epoch 131/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9342e-04\n",
      "Epoch 132/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8909e-04\n",
      "Epoch 133/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8481e-04\n",
      "Epoch 134/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8055e-04\n",
      "Epoch 135/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7631e-04\n",
      "Epoch 136/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7212e-04\n",
      "Epoch 137/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6795e-04\n",
      "Epoch 138/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6381e-04\n",
      "Epoch 139/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5970e-04\n",
      "Epoch 140/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5563e-04\n",
      "Epoch 141/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5158e-04\n",
      "Epoch 142/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4756e-04\n",
      "Epoch 143/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4357e-04\n",
      "Epoch 144/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.3962e-04\n",
      "Epoch 145/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3567e-04\n",
      "Epoch 146/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3177e-04\n",
      "Epoch 147/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2790e-04\n",
      "Epoch 148/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2406e-04\n",
      "Epoch 149/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2024e-04\n",
      "Epoch 150/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1645e-04\n",
      "Epoch 151/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1269e-04\n",
      "Epoch 152/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0895e-04\n",
      "Epoch 153/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0524e-04\n",
      "Epoch 154/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0156e-04\n",
      "Epoch 155/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9791e-04\n",
      "Epoch 156/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9428e-04\n",
      "Epoch 157/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9068e-04\n",
      "Epoch 158/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8710e-04\n",
      "Epoch 159/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8356e-04\n",
      "Epoch 160/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8003e-04\n",
      "Epoch 161/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7653e-04\n",
      "Epoch 162/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7306e-04\n",
      "Epoch 163/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6961e-04\n",
      "Epoch 164/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6620e-04\n",
      "Epoch 165/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6279e-04\n",
      "Epoch 166/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5942e-04\n",
      "Epoch 167/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5608e-04\n",
      "Epoch 168/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5275e-04\n",
      "Epoch 169/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4946e-04\n",
      "Epoch 170/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4618e-04\n",
      "Epoch 171/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4293e-04\n",
      "Epoch 172/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3971e-04\n",
      "Epoch 173/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3651e-04\n",
      "Epoch 174/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3333e-04\n",
      "Epoch 175/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3017e-04\n",
      "Epoch 176/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2703e-04\n",
      "Epoch 177/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2392e-04\n",
      "Epoch 178/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2083e-04\n",
      "Epoch 179/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1776e-04\n",
      "Epoch 180/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1472e-04\n",
      "Epoch 181/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1170e-04\n",
      "Epoch 182/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0870e-04\n",
      "Epoch 183/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0573e-04\n",
      "Epoch 184/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0276e-04\n",
      "Epoch 185/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9983e-04\n",
      "Epoch 186/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9692e-04\n",
      "Epoch 187/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9403e-04\n",
      "Epoch 188/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9116e-04\n",
      "Epoch 189/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8830e-04\n",
      "Epoch 190/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8547e-04\n",
      "Epoch 191/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8267e-04\n",
      "Epoch 192/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7988e-04\n",
      "Epoch 193/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7711e-04\n",
      "Epoch 194/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7437e-04\n",
      "Epoch 195/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7163e-04\n",
      "Epoch 196/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6894e-04\n",
      "Epoch 197/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6624e-04\n",
      "Epoch 198/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6357e-04\n",
      "Epoch 199/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6092e-04\n",
      "Epoch 200/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5830e-04\n",
      "Epoch 201/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5568e-04\n",
      "Epoch 202/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5309e-04\n",
      "Epoch 203/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5053e-04\n",
      "Epoch 204/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4797e-04\n",
      "Epoch 205/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4544e-04\n",
      "Epoch 206/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4292e-04\n",
      "Epoch 207/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4041e-04\n",
      "Epoch 208/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3794e-04\n",
      "Epoch 209/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3548e-04\n",
      "Epoch 210/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3303e-04\n",
      "Epoch 211/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3061e-04\n",
      "Epoch 212/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2820e-04\n",
      "Epoch 213/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2581e-04\n",
      "Epoch 214/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2343e-04\n",
      "Epoch 215/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2108e-04\n",
      "Epoch 216/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1874e-04\n",
      "Epoch 217/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1641e-04\n",
      "Epoch 218/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1411e-04\n",
      "Epoch 219/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1183e-04\n",
      "Epoch 220/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0956e-04\n",
      "Epoch 221/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0729e-04\n",
      "Epoch 222/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0506e-04\n",
      "Epoch 223/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0283e-04\n",
      "Epoch 224/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0063e-04\n",
      "Epoch 225/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9844e-04\n",
      "Epoch 226/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9626e-04\n",
      "Epoch 227/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9410e-04\n",
      "Epoch 228/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9196e-04\n",
      "Epoch 229/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8983e-04\n",
      "Epoch 230/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8772e-04\n",
      "Epoch 231/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8562e-04\n",
      "Epoch 232/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8355e-04\n",
      "Epoch 233/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8148e-04\n",
      "Epoch 234/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7943e-04\n",
      "Epoch 235/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7739e-04\n",
      "Epoch 236/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7537e-04\n",
      "Epoch 237/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7337e-04\n",
      "Epoch 238/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7137e-04\n",
      "Epoch 239/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6940e-04\n",
      "Epoch 240/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6743e-04\n",
      "Epoch 241/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6549e-04\n",
      "Epoch 242/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.6355e-04\n",
      "Epoch 243/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6163e-04\n",
      "Epoch 244/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5973e-04\n",
      "Epoch 245/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5783e-04\n",
      "Epoch 246/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5595e-04\n",
      "Epoch 247/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5409e-04\n",
      "Epoch 248/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5224e-04\n",
      "Epoch 249/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5040e-04\n",
      "Epoch 250/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4858e-04\n",
      "Epoch 251/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4677e-04\n",
      "Epoch 252/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4497e-04\n",
      "Epoch 253/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4318e-04\n",
      "Epoch 254/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4141e-04\n",
      "Epoch 255/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3965e-04\n",
      "Epoch 256/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3790e-04\n",
      "Epoch 257/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3618e-04\n",
      "Epoch 258/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3445e-04\n",
      "Epoch 259/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3275e-04\n",
      "Epoch 260/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3105e-04\n",
      "Epoch 261/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2936e-04\n",
      "Epoch 262/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2769e-04\n",
      "Epoch 263/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2604e-04\n",
      "Epoch 264/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2439e-04\n",
      "Epoch 265/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2275e-04\n",
      "Epoch 266/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2113e-04\n",
      "Epoch 267/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1952e-04\n",
      "Epoch 268/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1792e-04\n",
      "Epoch 269/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1633e-04\n",
      "Epoch 270/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1476e-04\n",
      "Epoch 271/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1319e-04\n",
      "Epoch 272/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1163e-04\n",
      "Epoch 273/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1009e-04\n",
      "Epoch 274/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0857e-04\n",
      "Epoch 275/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0705e-04\n",
      "Epoch 276/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0554e-04\n",
      "Epoch 277/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0404e-04\n",
      "Epoch 278/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0255e-04\n",
      "Epoch 279/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0107e-04\n",
      "Epoch 280/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9961e-04\n",
      "Epoch 281/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9816e-04\n",
      "Epoch 282/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9672e-04\n",
      "Epoch 283/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9528e-04\n",
      "Epoch 284/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9386e-04\n",
      "Epoch 285/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9245e-04\n",
      "Epoch 286/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9104e-04\n",
      "Epoch 287/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8965e-04\n",
      "Epoch 288/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8827e-04\n",
      "Epoch 289/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8690e-04\n",
      "Epoch 290/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8554e-04\n",
      "Epoch 291/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8419e-04\n",
      "Epoch 292/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8284e-04\n",
      "Epoch 293/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8151e-04\n",
      "Epoch 294/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8019e-04\n",
      "Epoch 295/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7888e-04\n",
      "Epoch 296/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7758e-04\n",
      "Epoch 297/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7628e-04\n",
      "Epoch 298/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7500e-04\n",
      "Epoch 299/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7372e-04\n",
      "Epoch 300/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7246e-04\n",
      "Epoch 301/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7120e-04\n",
      "Epoch 302/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6995e-04\n",
      "Epoch 303/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6872e-04\n",
      "Epoch 304/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6748e-04\n",
      "Epoch 305/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6627e-04\n",
      "Epoch 306/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6505e-04\n",
      "Epoch 307/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6385e-04\n",
      "Epoch 308/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6266e-04\n",
      "Epoch 309/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6147e-04\n",
      "Epoch 310/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6029e-04\n",
      "Epoch 311/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5913e-04\n",
      "Epoch 312/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5797e-04\n",
      "Epoch 313/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5681e-04\n",
      "Epoch 314/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5567e-04\n",
      "Epoch 315/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5454e-04\n",
      "Epoch 316/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5341e-04\n",
      "Epoch 317/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5230e-04\n",
      "Epoch 318/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5118e-04\n",
      "Epoch 319/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5009e-04\n",
      "Epoch 320/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4900e-04\n",
      "Epoch 321/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4790e-04\n",
      "Epoch 322/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4683e-04\n",
      "Epoch 323/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4576e-04\n",
      "Epoch 324/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4470e-04\n",
      "Epoch 325/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4364e-04\n",
      "Epoch 326/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4260e-04\n",
      "Epoch 327/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4156e-04\n",
      "Epoch 328/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4053e-04\n",
      "Epoch 329/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3950e-04\n",
      "Epoch 330/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3849e-04\n",
      "Epoch 331/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3747e-04\n",
      "Epoch 332/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3647e-04\n",
      "Epoch 333/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3548e-04\n",
      "Epoch 334/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3449e-04\n",
      "Epoch 335/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3352e-04\n",
      "Epoch 336/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3254e-04\n",
      "Epoch 337/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3158e-04\n",
      "Epoch 338/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3062e-04\n",
      "Epoch 339/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2967e-04\n",
      "Epoch 340/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2872e-04\n",
      "Epoch 341/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2778e-04\n",
      "Epoch 342/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2685e-04\n",
      "Epoch 343/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2593e-04\n",
      "Epoch 344/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2501e-04\n",
      "Epoch 345/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2410e-04\n",
      "Epoch 346/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2319e-04\n",
      "Epoch 347/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2230e-04\n",
      "Epoch 348/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2141e-04\n",
      "Epoch 349/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2052e-04\n",
      "Epoch 350/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1965e-04\n",
      "Epoch 351/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1877e-04\n",
      "Epoch 352/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1791e-04\n",
      "Epoch 353/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1705e-04\n",
      "Epoch 354/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1620e-04\n",
      "Epoch 355/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1535e-04\n",
      "Epoch 356/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1451e-04\n",
      "Epoch 357/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1367e-04\n",
      "Epoch 358/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1284e-04\n",
      "Epoch 359/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1203e-04\n",
      "Epoch 360/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1121e-04\n",
      "Epoch 361/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1039e-04\n",
      "Epoch 362/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0960e-04\n",
      "Epoch 363/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0879e-04\n",
      "Epoch 364/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0800e-04\n",
      "Epoch 365/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0722e-04\n",
      "Epoch 366/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0643e-04\n",
      "Epoch 367/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0566e-04\n",
      "Epoch 368/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0489e-04\n",
      "Epoch 369/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0412e-04\n",
      "Epoch 370/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0336e-04\n",
      "Epoch 371/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0261e-04\n",
      "Epoch 372/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0186e-04\n",
      "Epoch 373/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0112e-04\n",
      "Epoch 374/2000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0039e-04\n",
      "Epoch 375/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9655e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee70df6dd0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semkain tinggi epoch semakin tinggi nilai nya \n",
    "model.fit (x,y, epochs=2000, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediksi Data Baru "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EE6C4C8EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[[11.990896]\n",
      " [13.9856  ]\n",
      " [15.980304]]\n"
     ]
    }
   ],
   "source": [
    "# epoch 1000 + callbacks\n",
    "newData = np.array([6.0,7.0,8.0])\n",
    "\n",
    "y_pred = model.predict(newData)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasifikasi dengan Dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_raw, y_train_raw ),(x_test_raw, y_test_raw ) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Encoding Label pada Gambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 10\n",
    "\n",
    "# ONE-HOT ENCODING \n",
    "y_train = keras.utils.to_categorical(y_train_raw, num_class)\n",
    "y_test  = keras.utils.to_categorical(y_test_raw, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.imshow(x_train_raw[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_raw.shape\n",
    "# 28, 28 jumlah piksel\n",
    "# 60000 jumlah data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi 28x28 jadi sebuah vektor berukuran 784x1\n",
    "\n",
    "x_train = x_train_raw.reshape(60000,784)\n",
    "x_test = x_test_raw.reshape(10000, 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi piksel pada Gambar\n",
    "\n",
    "# 255 Total nilai RGB\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat DNN (Deep Neural Network)\n",
    "#### Terdiri dari 3 layers dan 2 aktivasi RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(512, activation='relu', input_dim = 784),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10,  activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 567,434\n",
      "Trainable params: 567,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimalisasi model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#jika cuma 2 shape atau 2 perbandingan seperti anjing dan kucing maka loss nya menggunakan binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0619 - accuracy: 0.9810\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0323 - accuracy: 0.9898\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0273 - accuracy: 0.9910\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0256 - accuracy: 0.9923\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0220 - accuracy: 0.9934\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0195 - accuracy: 0.9940\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0195 - accuracy: 0.9940\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0175 - accuracy: 0.9951\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0183 - accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0161 - accuracy: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee70f23220>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi model yang telas di train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9777\n",
      "TEST LOSS     :  0.12474387139081955\n",
      "TEST ACCURACY :  0.9776999950408936\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE = 32\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"TEST LOSS     : \", score[0])\n",
    "print(\"TEST ACCURACY : \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 6.0438e-04 - accuracy: 0.9999\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4590e-04 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.1307e-05 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 4.5135e-05 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.9516e-05 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.9969e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.3298e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 8.9110e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 5.9661e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee6d8d3a60>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9857\n",
      "TEST LOSS     :  0.11649435758590698\n",
      "TEST ACCURACY :  0.9857000112533569\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE = 128\n",
    "score2 = model.evaluate(x_test, y_test)\n",
    "print(\"TEST LOSS     : \", score2[0])\n",
    "print(\"TEST ACCURACY : \", score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bc3c14535ba198ac403d8b0a5a2d23428a5da826104b9a0c9c35d272c2c2677"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
